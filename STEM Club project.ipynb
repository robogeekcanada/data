{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Report_images/Robo-Geek_logo_Canada_150.PNG)\n",
    "\n",
    "# STEM Club Project -Fall 2017\n",
    "\n",
    "## Acknowledgments and Inspiration:\n",
    "\n",
    "Robo-Geek is very proud to present the Self Driving Car Simulator Project with STEM club. This couldn't be possible without the great opportunity to work with the open source community. \n",
    "\n",
    "Udacity open-sourced a self driving car simulator designed for Self Driving Car Nano Degree program.\n",
    "![alt text](Report_images/Udacity_self_driving_car_program.PNG)\n",
    "\n",
    "The objective of the project is to use the self driving car simulator to train a neural network by collecting images as we run and then use the model to run the simulator autonomously.\n",
    "\n",
    "Searching up for inspiration how to implement this project in Python, we run into this fantastic video from YouTuber star Siraj Raval with a github project that we used as the base of the STEM club project:\n",
    "\n",
    "![alt text](Report_images/Siraj_Raval.JPG)\n",
    "\n",
    "https://www.youtube.com/watch?v=EaY5QiZwSP4\n",
    "\n",
    "Again we are very thankful for the open source community, our project is shared at:\n",
    "\n",
    "https://github.com/robogeekcanada/data\n",
    "\n",
    "\n",
    "## High Level Understanding of the Project\n",
    "\n",
    "In this project we will use the Udacity simulator to conduct supervised training meaning we will drive the car and record our movements and collect the frames of the cameras inside the simulator. The images collected will be fed to a convolutional neural network, in addition augmentation of the images to simulate different conditions will be created and fed to the CNN. The design of the CNN is based on Nvidia's self driving car paper:\n",
    "\n",
    "https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "\n",
    "Finally with a trained CNN, we can drive the car autonomously. The CNN will decide on acceleration and steering. There are three Python programs that make this possible: model.py, utils. py and drive.py\n",
    "\n",
    "\n",
    "![alt text](Report_images/High_level_concept.JPG)\n",
    "\n",
    "Nine students ages 11-15 participated in this Robo-Geek project. They met weekly for 1.5 hours for 10 weeks. The project was divided in 2 parts:\n",
    "\n",
    "#### Part 1: Data adquisition with Udacity Simulator + Model training\n",
    "\n",
    "#### Part 2: Testing model with AWS servers and improvements to drive.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Adquisition with Udacity Simulator and Model Training\n",
    "\n",
    "\n",
    "### Udacity Simulator\n",
    "\n",
    "Udacity simulator has two modes: Training Mode and Autonomous mode:\n",
    "\n",
    "![alt text](Report_images/Udacity_simulator.JPG)\n",
    "\n",
    "\n",
    "The simulator world is built in Unity and new tracks can be added or changes to the physics as well. We only focused on the Lake race-track, however the same steps could be used to train on other tracks. Unity is open source free 3D modelling software, for more information https://unity3d.com/\n",
    "\n",
    "\n",
    "![alt text](Report_images/Unity_race_tracks.PNG)\n",
    "\n",
    "\n",
    "\n",
    "The code for the simulator can be found at:\n",
    "https://github.com/udacity/self-driving-car-sim\n",
    "\n",
    "\n",
    "### 1.1 Data Adquisition in Simulator Training Mode:\n",
    "\n",
    "The Udacity simulator can be configured and two race tracks are available. In this project we only train in the lake track.\n",
    "\n",
    "Once the simulator is opened under training mode, we can simply go around the track using keyboard, mouse or joystick.\n",
    "There is the option to record the information by pressing the Red Record button.\n",
    "\n",
    "![alt text](Report_images/Simulator_training.JPG)\n",
    "\n",
    "Three images are recorded per frame at different angles: LEFT, RIGHT and CENTER in a data folder and the log of where the images are stored is saved under driving_log.CSV\n",
    "\n",
    "![alt text](Report_images/data_folder.JPG)\n",
    "\n",
    "#### Left image:\n",
    "\n",
    "![alt text](Report_images/left.PNG)  \n",
    "\n",
    "#### Right image:\n",
    "\n",
    "![alt text](Report_images/right.PNG)\n",
    "\n",
    "#### Center image:\n",
    "\n",
    "![alt text](Report_images/center.PNG)\n",
    "\n",
    "#### Nvidia's Hardware design \n",
    "\n",
    "As per Nvidia's hardware design shown below:\n",
    "\n",
    "![alt text](Report_images/Nvidia_hardware_model.PNG)\n",
    "\n",
    "Records images from center, left, and right cameras w/ associated steering angle, speed, throttle and brake saves to driving_log.CSV.\n",
    "\n",
    "![alt text](Report_images/driving_log_csv.JPG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Model training using CNN-  Behavioral cloning\n",
    "\n",
    "\n",
    "The design and optimization of a convolutional network is outside the scope of this project. As mentioned previously, the project was inspired from Siraj Raval's project posted in who created a wrapper from the original project Naoki Shibuya:\n",
    "\n",
    "https://github.com/llSourcell/How_to_simulate_a_self_driving_car\n",
    "\n",
    "The 9 layer convolutional neural network consists of a 1 normalization layer, 5 convolutional layers and 3 connected layers that lead to output control vehicle as shown in the picture below:\n",
    "\n",
    "\n",
    "![alt text](Report_images/cnn_architecture.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    \"\"\"\n",
    "    NVIDIA model used\n",
    "    Image normalization to avoid saturation and make gradients work better.\n",
    "    Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
    "    Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
    "    Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
    "    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "    Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "    Drop out (0.5)\n",
    "    Fully connected: neurons: 100, activation: ELU\n",
    "    Fully connected: neurons: 50, activation: ELU\n",
    "    Fully connected: neurons: 10, activation: ELU\n",
    "    Fully connected: neurons: 1 (output)\n",
    "\n",
    "    # the convolution layers are meant to handle feature engineering\n",
    "    the fully connected layer for predicting the steering angle.\n",
    "    dropout avoids overfitting\n",
    "    ELU(Exponential linear unit) function takes care of the Vanishing gradient problem. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Conv2D(64, 3, 3, activation='elu'))\n",
    "    model.add(Dropout(args.keep_prob))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high amount of computation, a powerful computer is required. A single CPU may take several years to train the model. We decided to test different servers in Google Cloud and AWS. We decided to stick with AWS Windows Server, there is a server per every 2 students. Due to cost management we only turn them on when required. Ideally each student will have their own.\n",
    "\n",
    "\n",
    "![alt text](Report_images/AWS_Robo-Geek_servers.JPG)\n",
    "\n",
    "To connect to the servers we use Remote Desktop Connection, using the assigned user accounts:\n",
    "Anaconda is pre-installed for each user. And a copy of the project folder is stored in the Documents folder.\n",
    "\n",
    "![alt text](Report_images/Remote_Desktop_Connection.JPG)\n",
    "\n",
    "In this project we didn't cover how to install the environment for this project. More information can be found from the original project. Due to time constraint we preferred not to get into troubleshooting of installation. Let's just say it's not a smooth process all around.\n",
    "\n",
    "Using Conda Prompt, we execute the following commands in the project folder:\n",
    "\n",
    "![alt text](Report_images/training_model_conda.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the speed of the server training may take between an hour to a few hours. With the servers we selected it took 1 hour. The output of is model-<epoch>.h5 file that we will use to run in Autonomous Mode.\n",
    "\n",
    "In order to undestand what happens in CNN while it's training, there is an excellent paper by Naoki Shibuya:\n",
    "\n",
    "https://github.com/naokishibuya/car-behavioral-cloning\n",
    "\n",
    "It shows what happens in the Image augmentation process. The code that makes this happen is shown below under utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(data_dir, image_paths, steering_angles, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Generate training image give image paths and associated steering angles\n",
    "    \"\"\"\n",
    "    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n",
    "    steers = np.empty(batch_size)\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # argumentation\n",
    "            if is_training and np.random.rand() < 0.6:\n",
    "                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                image = load_image(data_dir, center) \n",
    "            # add the image and steering angle to the batch\n",
    "            images[i] = preprocess(image)\n",
    "            steers[i] = steering_angle\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                break\n",
    "        yield images, steers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augment function in utils.py calls the other functions that manipulate the images including random_flips, random_translates, random_shadow and random_brightness. As per the Nvidia's paper: \"we augment the data by adding artificial shifts and rotations to teach the network how to recover from a poor position or orientation. The magnitidue of these perturbations is chosen randomly from a normal distribution...Artificially augmenting the data does add undesirable artifacts as the magnitude increases.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augument(data_dir, center, left, right, steering_angle, range_x=100, range_y=10):\n",
    "    \"\"\"\n",
    "    Generate an augumented image and adjust steering angle.\n",
    "    (The steering angle is associated with the center image)\n",
    "    \"\"\"\n",
    "    image, steering_angle = choose_image(data_dir, center, left, right, steering_angle)\n",
    "    image, steering_angle = random_flip(image, steering_angle)\n",
    "    image, steering_angle = random_translate(image, steering_angle, range_x, range_y)\n",
    "    image = random_shadow(image)\n",
    "    image = random_brightness(image)\n",
    "    return image, steering_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"Report_videos/cnn training visualization.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Testing Model with AWS server and improving drive.py\n",
    "\n",
    "The processing speed required is significant, GPUs are recommended. Since we didn't have them available for all students, we deciced to use AWS servers. \n",
    "\n",
    "\n",
    "### 2.1 Testing the model with AWS\n",
    "\n",
    "All students were given an account to connect to an AWS server. We tested the following servers to see the impact of computing processing:\n",
    "\n",
    "![alt text](Report_images/AWS_servers.JPG)\n",
    "\n",
    "\n",
    "### 2.2 Improving drive.py\n",
    "\n",
    "We made small modifications to the existing drive.py to control the speed and augment the signal. We did not have time to balance the data. Please refer to final code drive5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from PIL import Image\n",
    "from flask import Flask\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import utils\n",
    "\n",
    "sio = socketio.Server()\n",
    "app = Flask(__name__)\n",
    "model = None\n",
    "prev_image_array = None\n",
    "\n",
    "MAX_SPEED = 10\n",
    "MIN_SPEED = 5\n",
    "\n",
    "speed_limit = MAX_SPEED\n",
    "\n",
    "Adjustment_factor = 1.2\n",
    "Speed_adjustment =0.8\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, data):\n",
    "    if data:\n",
    "        # The current steering angle of the car\n",
    "        steering_angle = float(data[\"steering_angle\"])\n",
    "        # The current throttle of the car\n",
    "        throttle = float(data[\"throttle\"])\n",
    "        # The current speed of the car\n",
    "        speed = float(data[\"speed\"])*Speed_adjustment\n",
    "        # The current image from the center camera of the car\n",
    "        image = Image.open(BytesIO(base64.b64decode(data[\"image\"])))\n",
    "        try:\n",
    "            image = np.asarray(image)       # from PIL image to numpy array\n",
    "            image = utils.preprocess(image) # apply the preprocessing\n",
    "            image = np.array([image])       # the model expects 4D array\n",
    "\n",
    "            # predict the steering angle for the image\n",
    "            steering_angle = float(model.predict(image, batch_size=1))*Adjustment_factor\n",
    "            # lower the throttle as the speed increases\n",
    "            # if the speed is above the current speed limit, we are on a downhill.\n",
    "            # make sure we slow down first and then go back to the original max speed.\n",
    "\n",
    "            if (steering_angle*steering_angle) > 0.04:\n",
    "                steering_angle = steering_angle*0.8\n",
    "\n",
    "                print(\"tight corner...\")\n",
    "    \n",
    "\n",
    "            global speed_limit\n",
    "            if speed > speed_limit:\n",
    "                speed_limit = MIN_SPEED  # slow down\n",
    "\n",
    "            else:\n",
    "                speed_limit = MAX_SPEED\n",
    "            #throttle = 1.0 - steering_angle**2 - (speed/speed_limit)**2\n",
    "            throttle = 0.18\n",
    "\n",
    "            print('{} {} {}'.format(steering_angle, throttle, speed))\n",
    "            send_control(steering_angle, throttle)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # save frame\n",
    "        if args.image_folder != '':\n",
    "            timestamp = datetime.utcnow().strftime('%Y_%m_%d_%H_%M_%S_%f')[:-3]\n",
    "            image_filename = os.path.join(args.image_folder, timestamp)\n",
    "            image.save('{}.jpg'.format(image_filename))\n",
    "    else:\n",
    "        # NOTE: DON'T EDIT THIS.\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    print(\"connect \", sid)\n",
    "    send_control(0, 0)\n",
    "\n",
    "\n",
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': steering_angle.__str__(),\n",
    "            'throttle': throttle.__str__()\n",
    "        },\n",
    "        skip_sid=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Remote Driving')\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        help='Path to model h5 file. Model should be on the same path.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'image_folder',\n",
    "        type=str,\n",
    "        nargs='?',\n",
    "        default='',\n",
    "        help='Path to image folder. This is where the images from the run will be saved.'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = load_model(args.model)\n",
    "\n",
    "    if args.image_folder != '':\n",
    "        print(\"Creating image folder at {}\".format(args.image_folder))\n",
    "        if not os.path.exists(args.image_folder):\n",
    "            os.makedirs(args.image_folder)\n",
    "        else:\n",
    "            shutil.rmtree(args.image_folder)\n",
    "            os.makedirs(args.image_folder)\n",
    "        print(\"RECORDING THIS RUN ...\")\n",
    "    else:\n",
    "        print(\"NOT RECORDING THIS RUN ...\")\n",
    "\n",
    "    # wrap Flask application with engineio's middleware\n",
    "    app = socketio.Middleware(sio, app)\n",
    "\n",
    "    # deploy as an eventlet WSGI server\n",
    "    eventlet.wsgi.server(eventlet.listen(('', 4567)), app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Testing mode\n",
    "\n",
    "With m4.x16large we run drive5.py in autonomous mode and success a complete lap!\n",
    "\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/1*nlusa_fC5BnsgnWPFnov7Q.tiff \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Further reading:\n",
    "\n",
    "Thanks to the wonderful open source community that is actively improving on this challenge, there are many other projects that beg to be reviewed that were not covered due to time constraint. These projects includes further analysis on data pre-processing, PID loops, discussion on best training techniques, not to mention other ideas to improve driving and predictions. Below a list of github projects that we used as references:\n",
    "\n",
    "https://github.com/Michael-Tu/Udacity-Self-Driving-Car/tree/master/p3-behavioral-cloning\n",
    "\n",
    "https://github.com/yazeedalrubyli/SDCND/tree/master/Term1/Behavioral-Cloning\n",
    "\n",
    "https://github.com/sibyjackgrove/Car_driving_Behaviour_cloning\n",
    "\n",
    "https://github.com/morganel/self-driving-cars/tree/master/behavioral-cloning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
